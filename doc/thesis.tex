% зазначаємо стильовий файл, який будемо використовувати
\documentclass{udstu}

\addbibresource{refs.bib}

% починаємо верстку документа
\begin{document}

\makediplomatitle{
% StudentName --- прізвище, ініціали студента
	StudentName={Скороденко Д. О.},
% StudentMale --- стать студента (true, якщо чоловік, false --- якщо жінка)
	StudentMale=true,
% StudentGroup --- група студента
	StudentGroup={КМ-01},
% Title --- назва
	Title={
		{\large Дипломна робота} \\
		на здобуття ступеня бакалавра \\
		за освітньо-професійною програмою \\
		\invcommas{Наука про дані та математичне моделювання} \\
		спеціальності 113 Прикладна математика \\
		на тему: \invcommas{Математичне та програмне забезпечення для автоматичного тегування зображень}
	},
	Department={ПМА},
	HeadOfDepartment={Чертов Олег Романович},
% SupervisorDegree --- науковий ступінь, учене звання керівника роботи
% якщо наукового ступеня немає, можна відповідний рядочок пропустити
	SupervisorDegree={доцент кафедри ПМА},
% SupervisorName --- прізвище, ініціали керівника роботи
	SupervisorName={Сирота С. В.},
	ConsultDegree={доцент кафедри ПМА},
	ConsultName={Мальчиков В. В.},
}

\begin{enumerate}[1.]
	\item Тема роботи \invcommas{Математичне та програмне забезпечення для автоматичного тегування зображень},
	науковий керівник роботи Сирота Сергій Вікторович, доцент кафедри ПМА, затверджені наказом по університету
	від \invcommas{...} ..... 2024 р. № ... .

	\item Термін подання студентом роботи \invcommas{...} червня 2024 р.

	\item Вихідні дані до роботи: розроблювана система для маркування зображень повинна
	забезпечити якість опису зображення краще або на рівні із існуючими рішеннями згідно із
	тестовими метриками.

	\item Зміст роботи: виконати огляд існуючих рішень задачі маркування зображення,
	провести моделювання системи на основі аналізу існуючих рішень, імплементація системи,
	тренування / тестування системи, аналіз ефективності компонентів системи,
	аналіз ефективності системи у порівнянні із існуючими рішеннями.

	\item Перелік обов'язкового ілюстративного матеріалу: діаграма архітектури композитної системи,
	розподіл лейблів у тренувальному/тестовому датасеті, числові характеристики датасету,
	графіки процесу тренування для кожної із компонент системи,
	таблиця для порівняння ефективності компонентів системи,
	таблиця для порівняння ефективносіт маркування у порівнянні з існуючими рішеннями,
	ілюстративні приклади роботи системи.

	\item Дата видачі завдання \invcommas{...} ... 2024 р.
\end{enumerate}

\begin{center}
	Календарний план
	\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
	\begin{longtable}{|P{0.05\textwidth}|P{0.4\textwidth}|P{0.25\textwidth}|P{0.15\textwidth}|}
			\hline
			№ з/п & Назва етапів виконання дипломної роботи & Термін виконання етапів роботи & Примітка \\
			\hline
			1.    & Вивчення та збір літератури за темою "маркування зображень" &
			25.01.2024 & + \\
			\hline
			2.    & Проведення аналізу особливостей існуючих систем маркування зображень &
			15.02.2024 & + \\
			\hline
			3.    & Вибір та підготовка датасету &
			25.02.2024 & + \\
			\hline
			4.    & Проектування системи маркування зображення &
			10.03.2024 & + \\
			\hline
			5.    & Проведення тестового тренування системи та відладка програми &
			25.03.2024 & + \\
			\hline
			6.    & Проведення валідації результатів тестового тренування &
			28.03.2024 & + \\
			\hline
			7.    & Проведення тренування фінальної версії системи &
			14.04.2024 & + \\
			\hline
			8.    & Проведення порівняльного аналізу компонентів системи &
			15.04.2024 & + \\
			\hline
			9.    & Проведення порівняння ефективності з існуючими рішеннями &
			15.04.2024 & + \\
			\hline
			10.    & Підготовка першої версії дипломної роботи &
			08.05.2024 & + \\
			\hline
			11.    & Оформлення пояснювальної записки &
			20.05.2024 & + \\
			\hline
	\end{longtable}

	\begin{tabularx}{\textwidth}{p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}}
		Студент & \_\_\_\_\_\_\_\_ & Дмитро СКОРОДЕНКО \\
		Керівник & \_\_\_\_\_\_\_\_ & Сергій СИРОТА \\
	\end{tabularx}
\end{center}



% Створюємо анотацію
\abstractUkr

Дипломну роботу виконано на 30 аркушах,
вона містить 3 додатки та перелік посилань на використані джерела з 22 найменувань.
У роботі наведено 8 рисунків та 3 таблиці.

\paragraph{\textbf{Акутальність теми:}}
В сучасному світі технології розвиваються надзвичайно швидко. Швидкість цього розвитку можна виміряти
об'ємами даних, яким оперуюють люди. Так для 2000-х років було цілком достатньо мати дискети із максимальним вмістом
до 10-15 МБ. Станом на 2024 рік, існують різні накопичувачі від 16 ГБ до декількох десятків ТБ. Чому це важливо?
Для того щоб переносити велику кількість даних потрібно, щоб генерувалось ще більше даних. І це дійсно так, якщо
наприклад розглянути сучасні хостинги зображень, бази даних медзакладів, бази даних супутникових знімків і тд.
то всюди ми побачимо уже від сотень тисяч до сотень мільйонів зображень,
при чому швидкість появи нових зображень стрімко зростає. Основними причинами такого росту є: збільшення кількості людей
та стрімка цифровізація більшості аспектів людської життєдіяльності. Такий вибуховий ріст у швидкості
появи нових зображень створює проблему структуризації зображень. Для вирішення цієї проблеми можна присвоїти
кожному зображенню лейбли, які загально описують його вміст.
Без таких лейблів будь яка стуктура, яка містить у собі велику кількість зображень перетвориться
у звалище. Саме тому важливо мати швидкий та якісний метод для автоматичного маркування зображень.

\paragraph{\textbf{Мета дослідження:}}
Метою даної роботи є розробка ПЗ для маркування зображень (шпалерів робочого столу)
для покращення системи категоріального пошуку зображень (шпалерів робочого столу).

\paragraph{\textbf{Завдання дослідження:}}
Створення системи, яка виконуватиме маркування шпалерів робочого столу на основі двох модальностей даних:
зображення та шумних тегів.

Для досягнення цієї мети було виконано наступні завдання:

\begin{itemize}
	\item Проведено аналіз існуючих рішень
	\item Змодельовано систему
	\item Проведено тренування системи
	\item Проведено аналіз ефективності компонентів системи
	\item Проведено порівняльний аналіз якісті і повноти опису розглянутої моделі
	відносно існуючих рішень на основі тестових метрик
	\item Проведено аналіз ілюстративних приклади роботи системи
\end{itemize}

\paragraph{\textbf{Об'єкт дослідження:}}
Об'єктом дослідження є маркування зображень, та методи покращення маркування зображення.
Для порівняння ефективності маркування серед існуючих рішень було обрано моделі,
для яких обраховані тестові метрики для того ж датасету, який обрано в даній роботі.
До множини існуючих рішень належать: SR-CNN-RNN \cite{sr-cnn-rnn}, Resnet-SRN \cite{srn},
MS-CMA \cite{cma}, Query2Label \cite{q2l}, Resnet-CPSD \cite{cpsd} та ін.

\paragraph{\textbf{Предмет дослідження:}}
Предметом дослідження є множина шпалерів робочого столу в якості основної модальності даних,
та додаткова інформація (надані людьми шумні теги) в якості додаткової.

\paragraph{\textbf{Методи дослідження:}}
\begin{itemize}[*]
	\item Теорія системного аналізу
	\item Проектування систем Data Science / Deep learning
	\item Проектування інформаційних систем
	\item Обробка та аналіз зображень та тегів на основі методів глибинного навчання
	\item Теорія алгоритмів
	\item Аналіз даних та математична статистика
\end{itemize}

\paragraph{\textbf{Кінцевий результат:}}
Кінцевим результатом даної роботи є математичне та програмне забезпечення, архітектура моделі,
вагові коефіцієнти натренованої моделі та код програмного забезпечення, в якому реалізовано дану роботу.

\paragraph{\textbf{Ключові слова:}}
Маркування зображень, глибинне навчання, нейронні мережі, згорткові нейронні мережі,
мультимодальні дані, багатошарові персептрони, композиція нейронних мереж.


\abstractEng

The thesis presented in 30 pages.
It contains 3 appendixes and bibliography of 22 references,
8 figures and 3 tables are given in the thesis.

\paragraph{\textbf{Topic relevance.}}
In modern world technologies are progressing with very high speed. The speed of this progress can be
measured by volume of stored data which people operate with. In this regard for the 2000s, it was
quite enough to have diskettes with the maximum volume of up to 10-15 MB. As of 2024, there are various
drives from 16GB to several tens of TB. Why is this important? In order to transfer large amounts of data,
even more data should be generated. And it really is, let's for example consider modern image hostings,
databases of medical institutions, databases of satelite images, etc. then everywhere we will see
from hundreds of thousands to hundreds of millions of images, and the rate of appearence of new images
is rapidly increasing. The main reasons for this growth are: an increase in the number of people globally and
the rapid digitalization of every aspect of our everyday lifes.
Such explosive growth in speed the appearance of new images creates a problem of image structuring.
To solve this problem, you can assign each image with labels that generally describe its content.
Without such labels, any structure that contains a large number of images will be like
a waste disposal site, where you can`t find anything.
That's why it's important to have a fast and quality method for automatic image labeling.

\paragraph{\textbf{Research goal:}}
The purpose of this work is to create deep learning model for automatic image labelling,
speccifically wallpapers, to improve categorical search using output labels.

\paragraph{\textbf{Research objectives:}}
Creating system, which will label images (wallpapers), given data of two modalities:
image and associated noisy tags.

To reach said objective the following tasks were completed:

\begin{itemize}
	\item Conducted analysis of existing solutions
	\item Modeled image labeling system
	\item Conducted model training
	\item Conducted analysis of performance gain from each component of composite system
	\item Conducted comparative analysis of model's labeling effectiveness
	with existing solutions
	\item Conducted analysis of illustrative examples of system's result
\end{itemize}

\paragraph{\textbf{Research object:}}
The object of research is image labeling and methods to imrove it.
To conduct comparative analysis to existing solution, only models which
are trained/tested on specific dataset were considered as 'existing solutions'.
To 'existing solutions' belong following models: SR-CNN-RNN \cite{sr-cnn-rnn}, Resnet-SRN \cite{srn},
MS-CMA \cite{cma}, Query2Label \cite{q2l}, Resnet-CPSD \cite{cpsd} etc.

\paragraph{\textbf{Research subject:}}
The subject of this work is subset of images which are called 'wallpapers'.
These are the images that you see as background image when you use your computer.
The main modality of date is image. Human provided tags would be additional modality.

\paragraph{\textbf{Research methods:}}
\begin{itemize}[*]
	\item System analysis
	\item Data Science / Deep learning
	\item Projecting of informational systems
	\item Processing and analysis of images and tags using methods of deep learning
	\item Theory of algorythms
	\item Data analysis and mathmatical statistics
\end{itemize}

\paragraph{\textbf{End result of research:}}
The end result of resarch is mathmatical and software implementation, architecure of model,
weights of trained models and source code of software which implements this work.

\paragraph{\textbf{Keywords:}}
Image labeling, deep learning, neural networks, convolutional neural networks, multimodal data,
multilayer perceptrons, composition of neural networks.


\shortings

\begin{itemize}[*]
	\item Модель - нейронна мережа
	\item Система - композиція нейронних мереж
	\item Задача класифікації - це задача, яка вирішує проблему приналежності чогось
	виключно до одного класу із довільного набору класів.
	\item Задача маркування - це задача, яка вирішує проблему приналежності чогось
	до декількох класів із довільного набору класів.
	\item Тегування та маркування - взаємозамінні поняття
	\item DNN - Глибинна нейронна мережа (Deep Neural Network)
	\item CNN - Згорткова нейронна мережа (Convolutional Neural Network)
	\item RNN - Рекурсивна нейронна мережа (Recursive Neural Network)
	\item ViT - Візуальні трансформери (Visual Transformers)
	\item Тег (Tag) - шумна інформація надана користувачем у формі тексту
	(наприклад для зображення кота "Cat, Canada, Cola")
	\item Лейбл (Label) - синонім слова ground truth для класифікації
\end{itemize}


% створюємо зміст
\tableofcontents


% створюємо перший розділ роботи
\chapter{Вступ}

Задача класифікації - це одна із основних задач в аналізі зображень, вона полягає
у присвоєнні кожному зображенню один із класів. Таким чином дане формулювання накладає
обмеження - зображення містить тільки один об'єкт. Поява DNN \cite{dnn-cls}
та її подальший розвитком у CNN \cite{cnn-cls-1,cnn-cls-2} разом із створенням
великих датасетів як-от ImageNet \cite{deng2009imagenet} дало змогу вирішувати задачу
класифікації зображень значно швидше і якісніше ніж люди.

Зрозуміло, що зображення - це той тип даних, який у абсолютній більшості випадків містить більше одного об'єкта,
і відповідно більше одного класу для класифікації.
Для поглиблення опису існує задача маркування зображень (image labeling).
На відміну від класифікації, вона полягає у маркуванні зображення більше ніж одним класом.
Таким чином повнота опису зображення кратно зростає у порівннянні із звичайною класифікацією,
однак привносить декілька складних завдань.

\begin{enumerate}[1)]
	\item Наявність декількох класів у одного зображення створює можливість
описувати значно ширший спектр візуальної інформації: різні об'єкти, стилі, дії, і тд.
Це створює потребу у розгляді додаткових джерел інформації, так як одного лиш зображення вже недостатньо.
Поява великих хостингів зображень таких як Imgur, Flickr, та ін., де користувачі можуть
як завантажувати різноманітні зображення, так і додавати до них описову інформацію у вигляді
тегів / анотацій, дала змогу створити досить різноманітні датасети: ImageNet \cite{deng2009imagenet},
MS-COCO \cite{cocodataset}, NUS-WIDE \cite{nus-wide-civr09}, та ін. Також існують і інші види датасетів,
наприклад: рентгенівські знімки та додаткова інформація (інші аналізи пацієнта, історія хвороб ...),
супутникові знімки та додаткова інформація у вигляді метаданих, геолокацій тощо.
Таким чином задача якісного маркування зображення вже охоплює значно більший спектр даних ніж просто
зображення.

	\item Маркування зображень передбачає динамічну к-сть промаркованих класів,
так для опису зображенням із широким спектром понять необхідно 5-6 класів,
для зображення із простим вмістом - 2-3 класи.

	\item Маркування зображень потребує оцінки якості проведеного маркування. Оскільки будь який
датасет буде містити в собі дизбаланс класів в тій чи іншій мірі, важливо оцінювати маркування із
урахуванням цього.

	\item Маркування зображень значно складніша задача ніж класифікація і відповідно зростає
складність моделей. З однієї сторони складніша модель потенційно здатна покращити якість маркування,
з іншої - може сильно збільшити як час на виконання маркування, так і час затрачений на тренування системи.
До складних систем належать ті, які використовують трансформери та/або мають велику к-сть параметрів.
Отже, важливо обрати певний баланс відносно складності моделі.
\end{enumerate}

Все це робить задачу маркування зображення досить складною.


\chapter{Огляд існуючих рішень задачі маркування зображення}

Розглянемо ключові аспекти задачі маркування зображення.

\section{Базове рішення}

Базовим рішенням в задачі маркування зображення є аналіз основної модальності даних - зображення.
У абсолютній більшості існуючих робіт використовується CNN (Convolutional Neural Network).
Застосовуються різні архітекури даної моделі ResNet \cite{resnet}, AlexNet \cite{alexnet},
GoogleNet \cite{googlenet}, ResNext \cite{resnext}.

В якості базового рішення також можна використовувати ViT \cite{vit}. Дана модель використовує
трансформери, і аналізує зображення по частинам (patches).

Спільним між CNN та ViT є те, зазвичай їх рідко коли тренують з нуля, так як для цього необхідно багато ресурсів.
Саме тому використовують вже натреновані (pretrained) моделі на великому датасеті,
здебільшого ImageNet \cite{deng2009imagenet}.
Для адаптації моделі до обраного контексту така модель дотреновуєтсья (fine tune), замінюючи існуючий класифікатор.
Це працює завдяки тому, що всі архітектури сучасних CNN та ViT моделей містять десятки мільйонів параметрів,
даючи широку репрезентацію зображень. Для CNN - це ієрархічне представлення: перші шари репрезентують базові
особливості, а останні - більш специфичні особливості. Для ViT - це представлення, яке будується
на основі взаємозв'язків між різними частинками зображення за допомогою трансформерів.
Такі репрезентації зображень дозволяють адаптовувати модель під різні задачі після проведення підгонки (finetune).

\section*{CNN (Convolutional Neural Networks)}

\subsection*{Основні характеристики:}

\begin{enumerate}
    \item \textbf{Архітектура:}
    \begin{itemize}
        \item Використовують згортки (convolutions) для обробки зображень.
        \item Складені з шарів згорток, активаційних функцій (наприклад, ReLU), пулінгу (max pooling або average pooling) та повнозв'язних шарів.
    \end{itemize}

    \item \textbf{Принцип роботи:}
    \begin{itemize}
        \item Згортки виділяють просторові особливості зображення (наприклад, краї, текстури).
        \item Кожен згортковий шар витягує ознаки вищого рівня по мірі проходження шарів (наприклад, краї $\rightarrow$ форми $\rightarrow$ об'єкти).
    \end{itemize}

    \item \textbf{Особливості:}
    \begin{itemize}
        \item Висока ефективність у задачах розпізнавання зображень.
        \item Використовують локальну інформацію через згортки з невеликими ядрами.
        \item Сильна локальна інваріантність, тобто можуть розпізнавати об'єкти незалежно від їхнього місця розташування в зображенні.
    \end{itemize}

    \item \textbf{Приклади моделей:}
    \begin{itemize}
        \item LeNet, AlexNet, VGG, ResNet, Inception.
    \end{itemize}
\end{enumerate}

\section*{ViT (Vision Transformers)}

\subsection*{Основні характеристики:}

\begin{enumerate}
    \item \textbf{Архітектура:}
    \begin{itemize}
        \item Базуються на архітектурі трансформерів, які були спочатку розроблені для обробки послідовностей у задачах обробки природної мови (наприклад, BERT, GPT).
        \item Складаються з шарів самоспрямованої уваги (self-attention) та повнозв'язних шарів.
    \end{itemize}

    \item \textbf{Принцип роботи:}
    \begin{itemize}
        \item Розбивають зображення на невеликі патчі (наприклад, 16x16 пікселів).
        \item Перетворюють кожен патч у вектор ознак за допомогою лінійної проекції.
        \item Додають позиційні кодування, щоб зберегти інформацію про розташування патчів.
        \item Використовують механізм самоспрямованої уваги для обробки глобальних залежностей між патчами.
    \end{itemize}

    \item \textbf{Особливості:}
    \begin{itemize}
        \item Добре працюють із глобальними залежностями в зображенні завдяки самоспрямованій увазі.
        \item Менше обмежені локальними взаємозв'язками в порівнянні з CNN.
        \item Потребують великої кількості даних для ефективного навчання.
    \end{itemize}

    \item \textbf{Приклади моделей:}
    \begin{itemize}
        \item Оригінальний Vision Transformer (ViT), DeiT (Data-efficient Image Transformers), Swin Transformer.
    \end{itemize}
\end{enumerate}

\section*{Порівняння}

\begin{itemize}
    \item \textbf{Локальна vs Глобальна обробка:}
    \begin{itemize}
        \item CNNs зосереджуються на локальних особливостях через згортки, тоді як ViT використовують глобальний контекст через самоспрямовану увагу.
    \end{itemize}

    \item \textbf{Ефективність:}
    \begin{itemize}
        \item CNNs можуть бути більш ефективними на невеликих наборах даних, тоді як ViT зазвичай потребують великих наборів даних та більше обчислювальних ресурсів для навчання.
    \end{itemize}

    \item \textbf{Простота інтерпретації:}
    \begin{itemize}
        \item Архітектура CNN часто вважається більш інтуїтивно зрозумілою через свою структуру шарів і використання згорток.
        \item ViT можуть бути складнішими для інтерпретації через складність механізму уваги.
    \end{itemize}

    \item \textbf{Гнучкість:}
    \begin{itemize}
        \item ViT є більш гнучкими в обробці довільних залежностей між частинами зображення, що може бути перевагою у складних задачах розпізнавання образів.
    \end{itemize}
\end{itemize}

Обидва підходи мають свої переваги та недоліки, і вибір між ними залежить від конкретної задачі,
доступних даних та обчислювальних ресурсів, так як ViT, попри всі свої переваги потребує в середньому
більше даних та ресурсів.

\section{Додаткова модальність даних}

Більш нові роботи також розглядають додаткові джерела інформації для підвищення якості та повноти
маркування зображень. Існує два основних підходи:

\begin{enumerate}
	\item \textit{Аналіз додаткової інформації.}
	Даний підхід аналізує додаткову до зображення інформацію.
	Це може бути як текстова інформація (теги / анотації) \cite{cnn-sinn,sr-cnn-rnn},
	так і метадані зображення \cite{cnn-neighbors,cnn-location}.
	Очевидним недоліком даного методу є потреба у цій додатковій інформації,
	яку можуть мати не всі зображення, а відсутність даної інформації
	знижує точність результуючого маркування.

	\item \textit{Аналіз цільових класів.}

	\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/resnet-cpsd-arch}
	\caption{
	Приклад архітектури моделі, яка використовує
	графове представлення цільових класів Resnet-CPSD \cite{cpsd}.
	}
	\label{figure:resnet-cpsd}
	\end{figure}

	На відміну від загальної інтерпретації класів для задачі маркування
	(коли кожен клас - це незалежна сутність), даний підхід аналізує зв'язки між цільовими класами,
	створючи нову модальність на основі набору цільових класів \cite{srn}.
	Більш новим та узагальненим підходом є технологія word2vec (або аналогічне рішення), яка дозволяє
	впорядкувати слова у певному векторному просторі таким чином, що їх просторове значення має
	прямий зв'язок із їх семантичним значенням. Наприклад модель Resnet-CPSD \cite{cpsd} використовує для аналізу класів
	граф, в якому вказуються класи (поняття) які часто знаходяться на одному зображенні
	(co-ocurrence; наприклад: риба, вода) та класи які рідко знаходяться
	на одному зображенні (dis-ocurrence; наприклад: риба, пустеля).
	Перевагою даного підходу є те, що йому не потрібні ніякі нові дані крім зображень та
	відповідних їм цільових класів, а нова модальність, яка репрезентує зв'язок між класами
	створюється під час тренування системи. Основним недоліком таких систем полягає у
	високій складності, і як наслідок - довше по часу тренування / розпізнавання.
\end{enumerate}

\clearpage

\section{Кількість лейблів}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/test-topk-144}
	\caption{
	Приклад адаптивної кількості лейблів,
	на основі роботи моделі на датасеті NUS-WIDE.
	'Truth' - правдиве маркування,
	'Top 5 pred' - ілюстрація вибору top $k$, при $k=5$,
	'Model pred' - приклад роботи моделі
	}
	\label{figure:test-topk}
\end{figure}

Результатом роботи класифікаційної моделі є вектор ймовірностей, який репрезентує приналежність до класів.
Для задачі класифікації вибір результату на основі цього вектора очевидний - клас із найбільшою ймовірністю, однак
для задачі маркування все складніше.
Блільшість наведених вище робоіт розглядаєють задачу вибору к-сті лейблів як найкращі $k$ (top $k$) маркувань (\chaptername{ \ref{metrics}}),
де $k$ - наперед задана константа. Очевидно, що такий вибір к-сті класів не є оптимальним,
так як більш змістовні зображення будуть містити менше описової інформації і навпапки - менш змістовні будуть
містити лишню інформацію, яка до того ж може не мати нічого спільного із цим зображенням (\figurename{ \ref{figure:test-topk}})

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/cnn-rnn-lqp}
	\caption{
	Приклад застосування алгоритму "beam search", для
	пошуку найбільш оптимального маркування зображення
	на основі найвищої ймовріності, який
	реалізовано у системі CNN-RNN \cite{cnn-rnn}.
	}
	\label{figure:cnn-rnn-lqp}
\end{figure}

Один із підходів як-от CNN-RNN \cite{cnn-rnn},
використовує RNN для аналізу візуальних даних (visual features) та автоматично виконує як задачу маркування,
так і задачу динамічного вибору кількості лейблів, однак в силу особливості RNN є певні обмеження
накладенні на порядок кодування класів. При чому важливо відмітити, що вибір динамічної кількості лейблів
за допомогою RNN має суттєвий недолік - залежність від балансу цільових класів у даних.

Найновіші моделі \cite{q2l, cpsd, cma}, які розглядають зв'язок між цільовими класами, обирають
к-сть цільових класів на основі порогового значення (\chaptername{ \ref{metrics}}). Варто зазначити, що
даний підхід є ефективним тільки для цього типу моделей, так як вектор ймовірностей, який отримується на виході
даної моделі досить сильно дискретизований,
тобто для позитивного лейблу, який маркуєтсья 1 ймовірність буде $\approx 0.7-0.9$,
а для негативного, тобто 0 ймовірність $\approx 0.1-0.3$.


\chapter{Моделювання}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{PNG/composite}
	\caption{Структура композитної системи}
	\label{figure:composite}
\end{figure}

На основі проведного аналізу альтернатив, дана робота пропонує розглянути
модель, яка розглядає дві модальності даних: зображення та текстові теги.
Структура даного рішення складається із чотирьох компонентів (\figurename{ \ref{figure:composite}}).


\section{Модель VCNN}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{PNG/vcnn}
	\caption{Архітектура моделі VCNN}
	\label{figure:vcnn}
\end{figure}

Модель VCNN (\figurename{ \ref{figure:vcnn}}) призначена для вивчення особливостей (features) із зображення.
Отримує на вхід піселі зображення $I$, у формі матриці розмірності $(B,C,W,H)$, де
$B$ - к-сть зображень у групі для тегування,
$C$ - к-сть каналів у зображеннях зазвичай 1 або 3, Grey або RGB відповідно,
$W,H$ - розмірність зображень.

За базове рішення використовуєтсья ResNext101\_32x8d \cite{resnext} (сучасна версія resnet),
із адаптованим класифікатором (vis feature extractor), натреновану на датасеті ImageNet \cite{deng2009imagenet}.

На виході даної моделі ми отримуємо вектор вірогідностей $vf$ (visual feature vector),
який вказує вірогідність маркування зображення класом $j$ на основі візуальної інформації.


\section{Модель MLP}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{PNG/mlp}
	\caption{Архітектура моделі MLP}
	\label{figure:mlp}
\end{figure}

MLP (\figurename{ \ref{figure:mlp}}) - аналізує текстові особливості (text features) тегів до зображення.
Теги до зображення $i$ репрезентуються як бінарний вектор $I = [1,0,1,0, ..., N]$,
де 1 - це наявність тегу, а $N$ - к-сть тегів.

Головна причина вибору звичайної MLP моделі для аналізу текстової інформації - це
те, що вхідна інформація - це шумні теги (наприклад: для фото кота - теги "Канада", "Кіт").

На виході даної моделі ми отримуємо вектор $tf$ вірогідностей (text feature vector),
який вказує вірогідність маркування зображення класом $j$ на основі текстової інформації.


\section{Модель LP}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{PNG/lp}
	\caption{Архітектура моделі LP}
	\label{figure:lp}
\end{figure}

LP (\figurename{ \ref{figure:lp}}) - аналізує вектор вірогідності $v$,
який є композицією векторів $vf$ та $tf$: $v = [vf, tf]$.

На виході даної моделі ми отримуємо вектор вірогідностей, який комбінує інформацію отриману як із візуальної так і з
текстової інформації.


\section{Модель LQP}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{PNG/lqp}
	\caption{Архітектура моделі LQP}
	\label{figure:lqp}
\end{figure}

Модель LQP (\figurename{ \ref{figure:lqp}}) аналізує кількість лейблів на основі вектору вірогідностей $v$,
який є композицією векторів $vf$ та $tf$: $v = [vf, tf]$.

Існує два підходи до визначення к-сті за допомогою нейронних мереж: класифікація та регресія.
LQP - регресійна модель.

Оскільки регресійні моделі досить швидко перенавчаються (overfitting), то необхідно задіяти регуляризацію.
В даній роботі, в якості регуляризатора задіяні Dropout шари \cite{dropout}, із вірогідністю відкидання (dropout rate) $0.5$.

На виході даної моделі є число, яке вказує на кількість лейблів у зображенні.


\section{Процес тренування}

Модель складається із декількох компонентів, що створює декілька проблемних місць під час тренування:
досить багато параметрів, дві різні цільові функції, проблема затухаючого градієнта, -
тому тренування відбуваєтсья у декілька стадій, у якому кожна із моделей тренується окремо
(деякі з них можна тренувати синхронно).


\paragraph{\textbf{Цільові функції}\\}

Для початку варто розглянути цільові функції (функція втрат, loss function).
Дані функції є базовим компонентом глибинного навчання.

В даній роботі використовуютсья дві функції: BCEWithLogitsLoss та MSELoss.

\paragraph{\textit{BCEWithLogitsLoss}\\}

Для тренування класифікаційних моделей (VCNN, MLP, LP) вихідні логіти $z_{ij}$
для групи (batch) зображень $I_N$ при $i = 1...N$, $j = 1...C$,
де $N$ - кількість зображень в групі, $C$ - кількість цільових класів,
цільова функція має вигляд:

\begin{equation}
\mathcal{L}_{cls} = \frac{1}{NC} \sum_{i}^{N} \sum_{j}^{C}
y_{ij} \cdot ln(\sigma(z_{ij})) + (1 - y_{ij}) \cdot ln(1 - \sigma(z_{ij}))
\end{equation}

, де $y_{ij} = 1$ якщо зображення $i$ анотоване класом $j$, інакше - $y_{ij} = 0$,
а $\sigma(\cdot)$ - це сигмоїдальна активаційна функція

\paragraph{\textit{MSELoss}\\}

Для тренування регресійної моделі LQP вихідні логіти $z_i$
для групи (batch) зображень $I_N$ при $i = 1...N$,
де $N$ - кількість зображень в групі,
цільова функція має вигляд:

\begin{equation}
\mathcal{L}_{reg} = \frac{1}{N} \sum_{i}^{N}
(y_i - z_i)^2
\end{equation}

, де $y_i$ - це кількість лейблів для зображення $I_i$.


\paragraph{\textbf{Тренування VCNN}\\}

Тренування моделі ResNext \cite{resnext} з нуля є досить складною задачою (дана модель має $\approx$ 80M параметрів),
адже для цього потрібні значні обчислювальні потужності.

Саме тому вироистовується натренована модель із адаптованим класифікатором (visual feature extractor) (\figurename{ \ref{figure:composite}}), яка підганяється (finetuned) на обраному датасеті.

Існує два підходи для підгонки:

\begin{enumerate}[1)]
	\item Підгонка всієї моделі:
	всі шари моделі підганяються (дотреновуються) з низькою швидкістю навчання (learning rate).
	Даний підхід вимагає великої обчислювальної потужності, однак надає високу точність та
	досить таки швидко тренується (в порівнянні із тренуванням з нуля).

	\item Підгонка класифікатора:
	відбуваєтсья тренування тільки класифікатора, фіксуючи всі інші параметри моделі.
	Даний підхід значно пришвидшує тренування в обмін на певну деградацію точності в порівнянні із 1-им варіантом.
\end{enumerate}

В даній роботі використовується 1 варіант підгонки, так як він надає вищу точність.


\paragraph{\textbf{Тренування MLP}\\}

Дана модель є звичайним багатошаровим персептроном, тому її тренування досить таки прямолінійне.


\paragraph{\textbf{Тренування LP}\\}

Дана модель призначена для обрахування вірогідностей на основі вектору $f = [vf, tf]$, оскільки вона
складається із одного шару то її тренування також очевидне.


\paragraph{\textbf{Тренування LQP}\\}

Дана модель є регресійною, її тренування також є очевидним, однак варто нормалізувати
вхідну к-сть лейблів, так як це пришвдшить та/або покращить збіжність моделі.


\section{Процес тестування}

\paragraph{\textbf{Тестування класифікаційних моделей (VCNN, MLP, LP)}\\}

Кожна із даних моделей обраховує вектор ймовірностей $P$,
для тестування необхідно перевести вектор ймовірностей (наприклад: $P = [0.9, 0.6, 0.1, 0.4, 0.6]$)
у вигляд маркування (наприклад: $M = [1,1,0,0,1]$). Дане перетворення називаєтсья індикаторною функцією.

\paragraph{Розглянемо два основних види індикаторної функції:}

\begin{enumerate}[1)]
	\item Порогове значення (threshold):
	для вектору ймовірностей $P$ та порогу $\alpha$ - вектор маркувань обраховується наступним чином:
	якщо $y_i > \alpha$, то маркуємо $1$, інакше $0$.
	Наприклад при $\alpha = 0.5$: $[0.9, 0.6, 0.1, 0.4, 0.6] \to [1,1,0,0,1]$.

	\item Найкращі $k$ (top $k$):
	для вектору ймовірностей $P$ та числа $k$ - вектор маркувань обраховується наступним чином:
	маркуємо $1$ найкращі $k$ ймовірностей, інакше $0$.
	Наприклад при $k = 4$: $[0.9, 0.6, 0.1, 0.4, 0.6] \to [1,1,0,1,1]$.
\end{enumerate}

Оскільки дані моделі \textbf{не передбачають} передбачення кількості лейблів,
то для тренування даних моделей використовується метод top $k$, при чому
$k = 3$.

\label{dynk}

Тобто для зображення $I$,
із маркуванням $Y = [1,0,0,0,1]$,
і вектором ймовірностей $P = [0.8,0.9,0.1,0.5,0.2]$
та результуючим вектором маркувань $M$:
$k = 3$, перетворення $P \to M \equiv [0.8,0.9,0.1,0.3,0.2] \to [1,1,0,1,0]$


\paragraph{\textbf{Тестування композитної моделі}\\}

Для тестування композитної моделі (\figurename{ \ref{figure:composite}})
необхідно обрахувати результуючі значення для моделей LP та LQP, і обрати
top $k$ лейблів LP, де $k$ - це передбачення LQP.


\chapter{Експерименти}

\section{Датасет}

Один із найбільш часто використовуваних датасетів для тестування моделей маркування зображень - NUS-WIDE \cite{nus-wide-civr09},
він складаєтсья із 269,655 зображень, 81 лейблу, та $\approx$ 5000 тегів в якості сторонньої текстової інформації.
Для проведення тренування/тестування використовується розподіл, надведений разом із
датасетом, так як він є збалансовний настільки, наскільки це можливо (\figurename{ \ref{figure:nus-wide-dist}}).

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/nus-wide-dist}
	\caption{Розподіл лейблів у тренувальному/тестовому датасеті}
	\label{figure:nus-wide-dist}
\end{figure}

Важливо відмітити, що даний датасет містить посиланя на зображення на ресурсі Flickr, і деякої частина цих
зображень вже там немає. Також буде використано тільки 1000 найбільш частих тегів з $\approx$ 5000,
при чому зображення, які не містять жодного тега відфільтровано.

Далі наведено деякі числові характеристики тренувальної/тестової вибірок:

\begin{center}
\begin{tabular}{ccc}
		\toprule[2pt]
           & Тренування & Тестування \\
        \midrule
        Кількість зобаржень & 121962  & 81636 \\
        Середня к-сть лейблів & 2.42  & 2.43 \\
        Медіана к-сть лейблів & 2  & 2 \\
        Мінімальна к-сть лейблів & 1  & 1 \\
        Максимальна к-сть лейблів & 12  & 13 \\
        \bottomrule[2pt]
\end{tabular}
\captionof{table}{Характеристики тренувальної/тестової вибірок}
\end{center}

\clearpage


\section{Метрики}
\label{metrics}

Для оцінки точності будуть використовуватись метрики, які є загально вживаними для оцінки
задачі маркування зображень (multi-label image annotation).

\begin{equation}
\begin{aligned}
\textnormal{С-P} &= \frac{1}{C} \sum_{j=1}^C \frac{NI^c_j}{NI^p_j} & \textnormal{O-P} &= \frac{\Sigma^N_{i=1} NL^c_i}{\Sigma^N_{i=1} NL^p_i} \\
\textnormal{C-R} &= \frac{1}{C} \sum_{j=1}^C \frac{NI^c_j}{NI^g_j} & \textnormal{O-R} &= \frac{\Sigma^N_{i=1} NL^c_i}{\Sigma^N_{i=1} NL^g_i} \\
\textnormal{C-F1} &= \frac{2 \cdot \textnormal{C-P} \cdot \textnormal{C-R}}{\textnormal{C-P} + \textnormal{C-R}} & \textnormal{O-F1} &= \frac{2 \cdot \textnormal{O-P} \cdot \textnormal{O-R}}{\textnormal{O-P} + \textnormal{O-R}} \\
\end{aligned}
\end{equation}

,де \begin{itemize}[*]
        \item $C$ - к-сть класів
        \item $N$ - к-сть тестових зображень
        \item $NI^c_j$ - к-сть зображень які \textbf{коректно} промарковано як клас $j$
        \item $NI^g_j$ - к-сть зображень які мають клас $j$
        \item $NI^p_j$ - к-сть зображень які промарковано як клас $j$
        \item $NL^c_i$ - к-сть \textbf{коректно} промаркованих лейблів для зображення $i$
        \item $NL^g_i$ - к-сть лейблів які має зображення $i$
        \item $NL^p_i$ - к-сть промаркованих лейблів для зображення $i$
\end{itemize}

Варто відзначити, що дані метрики є зміщеними (biased),
при чому по-класові метрики (C) зміщені в сторону рідкісних класів,
а загальні метрики (O) - в сторону частих класів \cite{cnn-labeling}.

\clearpage

Для того щоб отримати унфіормене представлення про ефективність моделі
буде використовуватись наступна метрика,
яка бере до уваги як C-F1 так і O-F1, що полегшує інтерпретацію результатів:

\begin{equation}
\text{H-F1} = \frac{2 \cdot \text{C-F1} \cdot \text{O-F1}}{\text{C-F1} + \text{O-F1}}
\end{equation}


\clearpage


\section{Тренування}

Для програмної реалізації запропонованої моделі було використано PyTorch.

Оскільки в даній роботі, використовується модель ResNext \cite{resnext} натренована
на датасеті ImageNet \cite{deng2009imagenet}, то для вхідних зображень потрібно
застосувати певне перетворення:

\begin{enumerate}[1)]
	\item Зміна розміру (Resize) $232 \times 232$,
	використовуючи білінійну інтерполяцію
	\item Центральний кроп (Central crop) $224 \times 224$
	\item Зміна масшатбу (Rescale) [0,1]
	\item Нормалізація на основі статистичних величин ImageNet \cite{deng2009imagenet}.
	А саме: mean = [0.485, 0.456, 0.406] та std = [0.229, 0.224, 0.225]
\end{enumerate}

Дане перетворення доступно у біблоітеці PyTorch.


\paragraph{\textbf{Параметри навчання}\\}

Класифікаційні моделі VCNN та MLP навчались із
швидкістю навання (learning rate) 0.001, а LP - зі швидкістю 0.01.
Також для начання цих трьох моделей використовувався контроллер швикдості навчання
(learning rate scheduler), який множив швидкість навчання на 0.5, досягаючи
5-ої та 10-ої епохи.

Регресійна модель LQP навчалась із сталою швидкістю навчання (learning rate) 0.0005.

Для всіх навчання всіх вище згаданих моделей використовувався оптимізатор
AdamW, із параметром l1 регуляризації (weight decay) 0.0003.

Розмір групи (batch size) 32.

Також варто відзначити, що в даній роботі епоха - це 20\% від усіх даних,
при чому після кожної епохи дані перемішуються (shuffle), отримаючи
нові 20\% даних.


\paragraph{\textbf{Процес навчання}\\}

Для тренування було використано графічний процесор 'Nvidia L4'.
Для тренування всіх елементів моделі знадобилось $\approx$ 3 години.

Для оптимізації процесу тренування було застосовано техніку mixed precision,
яка використовує f16 замість f32, під час певних етапів тренування \cite{mixed-precision}.


\begin{figure}[h]
\centering
	\subcaptionbox{Навчання VCNN}
		{\includegraphics[width=0.495\linewidth]{PNG/vcnn-train}}
	\subcaptionbox{Навчання MLP}
		{\includegraphics[width=0.495\linewidth]{PNG/mlp-train}}
	\subcaptionbox{Навчання LP}
		{\includegraphics[width=0.495\linewidth]{PNG/lp-train}}
	\subcaptionbox{Навчання LQP}
		{\includegraphics[width=0.495\linewidth]{PNG/lqp-train}}
\caption{Процес навчання моделей}
\end{figure}

\chapter{Аналіз результатів}

\section{Аналіз компонентів системи}

\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{l|c|c|ccc|ccc|c}
		\toprule[2pt]
		Модель & Індикаторна ф-ція (\ref{dynk}) & Модальність & C-P & C-R & C-F1 & O-P & O-R & O-F1 & H-F1 \\
        \midrule
        Композитна  & top k & Зображення+теги & 72.49 & 59.51 & 65.36 & 76.53 & 74.41 & 75.46 & 70.04 \\
        VCNN+MLP+LP & top 3 & Зображення+теги & 60.51 & 61.28 & 60.89 & 67.87 & 71.52 & 63.98 & 62.40 \\
        VCNN+LQP    & top k & Зображення  & 64.62 & 37.61 & 47.54 & 77.07 & 59.01 & 66.84 & 55.56 \\
		VCNN        & top 3 & Зображення  & 44.48 & 53.32 & 48.50 & 55.44 & 68.52 & 61.29 & 54.15 \\
        \bottomrule[2pt]
\end{tabular}}
\captionof{table}{Порівняння компонентів моделі}
\end{center}

Результуюча композитна модель показала значно кращий результат ніж базове рішення (VCNN).

\paragraph{\textbf{Додаткова модальність (MLP+LP)}\\}

Додавання додаткової модальності у вигляді тегів внесло значний вклад
у підвищення якості маркування.
Порівнюючи відповідні метрики H-F1 для моделей VCNN та VCNN+MLP+LP, можна
побачити приріст на $8.25\%$. При чому варто відзначити, що цей ріст в основному
забезпечений приростом метрики C-F1, яка є зміщеною в сторону більш рідкісних класів.
Варто відзначити, що це працює завдяки тому що зазвичай теги надані користувачами відмічають
досить рідкісні поняття, які на відміну від частих тегів (небо, сонце, людина, вода і тд.) складно
розпізнати маючи одне лиш зображення.

\paragraph{\textbf{Передбачення кількості лейблів (LQP)}\\}

При використанні компоненту LQP кількість лейблів обирається за принципом 'top k', а не 'top 3' (\ref{dynk}).
Це очевидним чином підвищує точність фінального  маркування, адже деякі зображення можуть мати більше трьох лейблів,
інші - менше трьох. Порівнюючи вплив компоненти LQP для базового рішення (VCNN) та композитної моделі (VCNN+MLP+LP+LQP)
можна зробити припущення, що VCNN аналізує загальні поняття на зображенні, а враховуючи дизбаланс класів у датасеті,
використання принципу 'top k' збільшує точність (precision), сильно жертвуючи по-класовим охопленням (C-R), і,
як наслідок, не сильно збільшуючи величину головної метрики H-F1. На практиці це виливалось у те, абсолютна більшість
зображень маркувалась частими лейблами (людина, вода, небо і тд.), а рідкісні теги - ігнорувались.
Натомість у композитній моделі вищезгаданий принцип чудово проявив себе.
Згідно із тестовими метриками покращення становить $7.64\%$.


\section{Порівнняня з існуючими рішеннями}

\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{l|c|c|ccc|ccc|c}
	\toprule[2pt]
	Модель & Індикаторна ф-ція (\ref{dynk}) & Модальність & C-P & C-R & C-F1 & O-P & O-R & O-F1 & H-F1 \\
	\midrule
	Композитна  & top k & Зображення+теги & 72.49 & 59.51 & 65.36 & 76.53 & 74.41 & 75.56 & 70.04 \\
	\hline
	Query2Label \cite{q2l} & threshold $\alpha$ & Зображення (+аналіз класів) & - & - & 67.60 & - & - & 76.3 & 71.69 \\
	SR-CNN-RNN \cite{sr-cnn-rnn} & top 3 & Зображення+теги & 71.73 & 61.73 & 66.36 & 77.41 & 76.88 & 77.15 & 71.35 \\
	Resnet-CPSD \cite{cpsd} & threshold $\alpha$ & Зображення (+аналіз класів) & - & - & 64.00 & - & - & 75.30 & 69.19 \\
	MS-CMA \cite{cma} & threshold $\alpha$ & Зображення (+аналіз класів) & - & - & 60.50 & - & - & 73.80 & 66.49 \\
	Resnet-SRN \cite{srn} & threshold 0.5 & Зображення (+аналіз класів) & 65.20 & 55.80 & 58.50 & 75.50 & 71.50 & 73.40 & 65.10 \\
	SINN \cite{cnn-sinn}  & top 3 & Зображення+теги & 58.30 & 60.63 & 59.44 & 57.05 & 79.12 & 66.29 & 62.68 \\
	TagNeighbour \cite{cnn-neighbors}  & top 3 & Зображення+метадані & 54.74 & 57.30 & 55.99 & 53.46 & 75.10 & 62.46 & 59.05 \\
	CNN+Logistic \cite{cnn-sinn}  & top 3 & Зображення  & 45.60 & 45.03 & 45.31 & 51.32 & 70.77 & 59.50 & 51.44 \\
	CNN-RNN \cite{cnn-rnn}  & top 3 & Зображення  & 40.50 & 30.40 & 34.70 & 49.9 & 61.70 & 55.20 & 42.61 \\
	CNN+WARP \cite{cnn-labeling}  & top 3 & Зображення  & 31.65 & 35.60 & 33.51 & 48.59 & 60.49 & 53.89 & 41.32 \\
	CNN+Softmax \cite{cnn-labeling}  & top 3 & Зображення  & 31.68 & 31.22 & 31.45 & 47.82 & 59.52 & 53.03 & 39.48 \\
	\bottomrule[2pt]
\end{tabular}}
\label{table:models-metrics}
\captionof{table}{Порівняння результуючих метрик для різних моделей на датасеті NUS-WIDE}
\end{center}


\paragraph{\textbf{Точність запропонованого рішення}}
Композитна модель (VCNN+MLP+LP+LQP) продемонструвала високу якість маркування на тестових метриках у порівнянні
із розглянутими альтернативними рішеннями. Згідно із метрикою H-F1 запропоноване рішення є третім.

\paragraph{\textbf{Модальність даних}}
Задача маркування зображень розглядає зображення як основну модальність, однак додавання модальності,
очікувано, покращує результати маркування. Це підтверджують метрики наведені в \tablename{ \ref{table:models-metrics}}.
Серед розглянутих рішень є 3 варіанти модальності даних з якимим працюють нейронні мережі.
Найменш ефективним, як і очікувалось, виявились моделі які аналізують виключно зображення.
Введення інших двох видів додаткових модальностей: теги, аналіз класів, - надають значно кращі результати.
Варто відзначити, - аналіз класів (найкраще імплементовано в: Query2Label \cite{q2l}, Resnet-CPSD
\cite{cpsd} та MS-CMA \cite{cma}) не потребує ніяких додактових даних окрім зображення,
що є вагомою конкурентною перевагою, враховуючи незначну відміність в точності моделей.

\paragraph{\textbf{Індикаторна функція}}
Для оцінки ефективності запропонованої індикаторної функції 'top k', варто ізолювати вплив саме цієї фнкції.
Для ціього розглянемо існуючі моделі, які працюють із тією ж модальністю даних. Найкращою із таких моделей є
SINN \cite{cnn-sinn}. Використання під-системи LQP, яка передбачає роботу із динамічною кількістю лейблів при маркуванні
(top k) значено підвищує якість. Згідно із наведеними метриками покращення складає
$7.36\%$ (\tablename{ \ref{table:models-metrics}}), що є вагомим приростом.


\clearpage

\section{Демонстративні приклади}

З тестового датасету випадковим чином обрано декілька зображень, для демонстрації роботи моделі:

\begin{figure}[h]
\centering
	\subcaptionbox{}
		{\includegraphics[width=0.495\linewidth]{PNG/showcase-1-20}}
		\label{figure:showcase-1}
	\subcaptionbox{}
		{\includegraphics[width=0.495\linewidth]{PNG/showcase-2-30}}
		\label{figure:showcase-2}
	\subcaptionbox{}
		{\includegraphics[width=0.495\linewidth]{PNG/showcase-3-40}}
 	\subcaptionbox{}
 		{\includegraphics[width=0.495\linewidth]{PNG/showcase-4-70}}
\caption{Демонстративні приклади}
\end{figure}

Задамо умовне позначення "а::1", що означає демонстративний приклад "а", перше зображення зверху,
"в::2" - приклад "в", друге зображення зверху і тд.

Серед наведених прикладів можна розглянути кілька цікавих моментів, які не відображають тестові метрики:

\begin{itemize}[*]
	\item Іноді модель передбачає маркування, якого немає у датасеті, однак присутнє на зображені.
	Наприклад: [а::3,а::4,б::1,б::3,б::4,в::1,в::2,г::2,г::3].
	\item Існують випадки коли модель відмічає поняття, які не можуть бути присутніми на одному зображенні.
	Це є прямим наслідком того, що наша модель розглядає цільові класи як незалежі сутності.
	Наприклад: lake та ocean як-от в прикладі 'в::3'.
	\item Іноді передбачення моделі відсікають неіснуючі поняття та маркують зображення краще ніж це
	було зроблено в датасеті. Так для зображення 'б::3' на якому зображено якусь рослину датасет
	вказує що це: 'buildings, clouds'; а модель - 'flowers'.
\end{itemize}

Окрім наведених вище особливих випадків, іноді модель, звичайно, помиляється. Однак у наведених
прикладах немає значних помилок у маркуванні.

% створюємо Висновки
\conclusions

В даній роботі було розглянуто композитну модель, для маркування зображень (шпалерів робочого столу)
проведено оцінювання її точності на тестових метриках.

Розглянута модель показала хороший результат у порівнянні із існуючими альтернативними рішеннями.

\textit{До переваг розглянутого рішення належать:}

\begin{itemize}[+]
	\item Висока точність
	\item Невелика кількість параметрів ($\approx 95\%$ параметрів має модель для аналізу зображень)
	\item Висока швидкість тренування
\end{itemize}

\textit{Недоліками є:}

\begin{itemize}[-]
	\item Неможливість тренування моделі в один етап (end-to-end)
	\item Необхідність використання додаткових даних (тегів) для
	отримання високої якості опису зображення
\end{itemize}

В подальшому варто розглянути ефективність даної моделі на інших датасетах (наприклад MSCOCO \cite{cocodataset}).
також варто розглянути і методи для аналізу цільових класів, так як незважаючи на
значне ускладнення фінальної моделі це надає досить високий приріст до точності маркування.

Результуюча композитна модель збережена у форматі safetensors.

Після проведеного дослідження було висунуто гіпотезу, що дане рішення можна застосувати
і у інших схожих предметних областях. Так для аналізу рентгенівських знімків - це може бути історія хвороб пацієнта,
для супутникових знімків - різні метадані, геолокація тощо, а для аналізу звичайних фотографій
- теги, анотації, метадані, тощо.

\printbibliography

\append{Код лістинг}

Код міститься у публічному github репозиторії
\underline{\href{https://github.com/skorodenko/wallpaper\_tagging}{посилання}}

Далі наведено загальний опис елементів проекту:


\paragraph{\textbf{Дані:}\\}

Файли в директорії scripts 'nuswide2ndjson.py' та '1ktags.py' призначенні для обробки
сирих даних із датасету в формат ndjson для подальшого тренування.

Файл data.py містить адаптери та визначення датасету для тренування.


\paragraph{\textbf{Тренування:}\\}

Скрипти для тренування для всіх моделей (VCNN, MLP, LP, LQP) знаходяться в директорії
'scripts/train'.


\paragraph{\textbf{Тестування:}\\}

Скрипт для тестування моделей: 'scripts/test.py'.
Даний скрипт передбачає тестування як фінальної моделі, так і деяких конфігурацій її компонентів.


\paragraph{\textbf{Інше:}\\}

Скрипт 'scripts/compose2safe.py' призначений для конвертації вагів моделей (VCNN, MLP, LP, LQP)
формату .ckpt у композитну модель формату .safetensors.

Ноутбук 'testing.ipynb' призначений для наглядного тестування моделі.


\append{Додаткові приклади}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-100}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-125}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-150}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-175}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-200}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-225}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-250}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-275}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-300}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-325}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-350}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-375}
	\caption{}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/showcase-400}
	\caption{}
\end{figure}

\append{Ілюстративний матеріал}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-1}
	\caption{Слайд 1}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-2}
	\caption{Слайд 2}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-3}
	\caption{Слайд 3}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-4}
	\caption{Слайд 4}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-5}
	\caption{Слайд 5}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-6}
	\caption{Слайд 6}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-7}
	\caption{Слайд 7}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-8}
	\caption{Слайд 8}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-9}
	\caption{Слайд 9}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-10}
	\caption{Слайд 10}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-11}
	\caption{Слайд 11}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-12}
	\caption{Слайд 12}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-13}
	\caption{Слайд 13}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-14}
	\caption{Слайд 14}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-15}
	\caption{Слайд 15}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-16}
	\caption{Слайд 16}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-17}
	\caption{Слайд 17}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-18}
	\caption{Слайд 18}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-19}
	\caption{Слайд 19}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-20}
	\caption{Слайд 20}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-21}
	\caption{Слайд 21}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-22}
	\caption{Слайд 22}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-23}
	\caption{Слайд 23}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-24}
	\caption{Слайд 24}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-25}
	\caption{Слайд 25}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-26}
	\caption{Слайд 26}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-27}
	\caption{Слайд 27}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{PNG/present/present-28}
	\caption{Слайд 28}
\end{figure}

\end{document}
