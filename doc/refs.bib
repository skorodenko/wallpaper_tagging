@INPROCEEDINGS{dnn-cls,
  author={Ciregan, Dan and Meier, Ueli and Schmidhuber, Jürgen},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  title={Multi-column deep neural networks for image classification},
  year={2012},
  volume={},
  number={},
  pages={3642-3649},
  keywords={Training;Error analysis;Neurons;Computer architecture;Benchmark testing;Graphics processing unit},
  doi={10.1109/CVPR.2012.6248110}
}

@ARTICLE{cnn-cls-1,
  title         = "Very deep convolutional networks for large-scale image
                   recognition",
  author        = "Simonyan, Karen and Zisserman, Andrew",
  abstract      = "In this work we investigate the effect of the convolutional
                   network depth on its accuracy in the large-scale image
                   recognition setting. Our main contribution is a thorough
                   evaluation of networks of increasing depth using an
                   architecture with very small (3x3) convolution filters,
                   which shows that a significant improvement on the prior-art
                   configurations can be achieved by pushing the depth to 16-19
                   weight layers. These findings were the basis of our ImageNet
                   Challenge 2014 submission, where our team secured the first
                   and the second places in the localisation and classification
                   tracks respectively. We also show that our representations
                   generalise well to other datasets, where they achieve
                   state-of-the-art results. We have made our two
                   best-performing ConvNet models publicly available to
                   facilitate further research on the use of deep visual
                   representations in computer vision.",
  month         =  sep,
  year          =  2014,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1409.1556"
}

@ARTICLE{cnn-cls-2,
  title         = "An Introduction to Convolutional Neural Networks",
  author        = "O'Shea, Keiron and Nash, Ryan",
  abstract      = "The field of machine learning has taken a dramatic twist in
                   recent times, with the rise of the Artificial Neural Network
                   (ANN). These biologically inspired computational models are
                   able to far exceed the performance of previous forms of
                   artificial intelligence in common machine learning tasks.
                   One of the most impressive forms of ANN architecture is that
                   of the Convolutional Neural Network (CNN). CNNs are
                   primarily used to solve difficult image-driven pattern
                   recognition tasks and with their precise yet simple
                   architecture, offers a simplified method of getting started
                   with ANNs. This document provides a brief introduction to
                   CNNs, discussing recently published papers and newly formed
                   techniques in developing these brilliantly fantastic image
                   recognition models. This introduction assumes you are
                   familiar with the fundamentals of ANNs and machine learning.",
  month         =  nov,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1511.08458"
}

@INPROCEEDINGS{cnn-sinn,
  author={Hu, Hexiang and Zhou, Guang-Tong and Deng, Zhiwei and Liao, Zicheng and Mori, Greg},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={Learning Structured Inference Neural Networks with Label Relations},
  year={2016},
  volume={},
  number={},
  pages={2960-2968},
  keywords={Visualization;Neural networks;Predictive models;Computational modeling;Semantics;Image segmentation;Correlation},
  doi={10.1109/CVPR.2016.323}
}

@INPROCEEDINGS {cnn-neighbors,
author = {J. Johnson and L. Ballan and L. Fei-Fei},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
title = {Love Thy Neighbors: Image Annotation by Exploiting Image Metadata},
year = {2015},
volume = {},
issn = {2380-7504},
pages = {4624-4632},
abstract = {Some images that are difficult to recognize on their own may become more clear in the context of a neighborhood of related images with similar social-network metadata. We build on this intuition to improve multilabel image annotation. Our model uses image metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities, then uses a deep neural network to blend visual information from the image and its neighbors. Prior work typically models image metadata parametrically, in contrast, our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing. We perform comprehensive experiments on the NUS-WIDE dataset, where we show that our model outperforms state-of-the-art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata.},
keywords = {metadata;visualization;training;vocabulary;computational modeling;adaptation models;social network services},
doi = {10.1109/ICCV.2015.525},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2015.525},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {dec}
}

@INPROCEEDINGS {cnn-location,
author = {K. Tang and M. Paluri and L. Fei-Fei and R. Fergus and L. Bourdev},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
title = {Improving Image Classification with Location Context},
year = {2015},
volume = {},
issn = {2380-7504},
pages = {1008-1016},
abstract = {With the widespread availability of cellphones and cameras that have GPS capabilities, it is common for images being uploaded to the Internet today to have GPS coordinates associated with them. In addition to research that tries to predict GPS coordinates from visual features, this also opens up the door to problems that are conditioned on the availability of GPS coordinates. In this work, we tackle the problem of performing image classification with location context, in which we are given the GPS coordinates for images in both the train and test phases. We explore different ways of encoding and extracting features from the GPS coordinates, and show how to naturally incorporate these features into a Convolutional Neural Network (CNN), the current state-of-the-art for most image classification and recognition problems. We also show how it is possible to simultaneously learn the optimal pooling radii for a subset of our features within the CNN framework. To evaluate our model and to help promote research in this area, we identify a set of location-sensitive concepts and annotate a subset of the Yahoo Flickr Creative Commons 100M dataset that has GPS coordinates with these concepts, which we make publicly available. By leveraging location context, we are able to achieve almost a $7\%$ gain in mean average precision.},
keywords = {global positioning system;tagging;twitter;context;internet;image recognition;snow},
doi = {10.1109/ICCV.2015.121},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2015.121},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {dec}
}

@ARTICLE{cpsd,
  title         = "Boosting multi-Label Image Classification with complementary
                   Parallel Self-distillation",
  author        = "Xu, Jiazhi and Huang, Sheng and Zhou, Fengtao and Huangfu,
                   Luwen and Zeng, Daniel and Liu, Bo",
  abstract      = "Multi-Label Image Classification (MLIC) approaches usually
                   exploit label correlations to achieve good performance.
                   However, emphasizing correlation like co-occurrence may
                   overlook discriminative features of the target itself and
                   lead to model overfitting, thus undermining the performance.
                   In this study, we propose a generic framework named Parallel
                   Self-Distillation (PSD) for boosting MLIC models. PSD
                   decomposes the original MLIC task into several simpler MLIC
                   sub-tasks via two elaborated complementary task
                   decomposition strategies named Co-occurrence Graph Partition
                   (CGP) and Dis-occurrence Graph Partition (DGP). Then, the
                   MLIC models of fewer categories are trained with these
                   sub-tasks in parallel for respectively learning the joint
                   patterns and the category-specific patterns of labels.
                   Finally, knowledge distillation is leveraged to learn a
                   compact global ensemble of full categories with these
                   learned patterns for reconciling the label correlation
                   exploitation and model overfitting. Extensive results on
                   MS-COCO and NUS-WIDE datasets demonstrate that our framework
                   can be easily plugged into many MLIC approaches and improve
                   performances of recent state-of-the-art approaches. The
                   explainable visual study also further validates that our
                   method is able to learn both the category-specific and
                   co-occurring features. The source code is released at
                   https://github.com/Robbie-Xu/CPSD.",
  month         =  may,
  year          =  2022,
  copyright     = "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2205.10986"
}


@ARTICLE{q2l,
  title         = "{Query2Label}: A Simple Transformer Way to {Multi-Label}
                   Classification",
  author        = "Liu, Shilong and Zhang, Lei and Yang, Xiao and Su, Hang and
                   Zhu, Jun",
  abstract      = "This paper presents a simple and effective approach to
                   solving the multi-label classification problem. The proposed
                   approach leverages Transformer decoders to query the
                   existence of a class label. The use of Transformer is rooted
                   in the need of extracting local discriminative features
                   adaptively for different labels, which is a strongly desired
                   property due to the existence of multiple objects in one
                   image. The built-in cross-attention module in the
                   Transformer decoder offers an effective way to use label
                   embeddings as queries to probe and pool class-related
                   features from a feature map computed by a vision backbone
                   for subsequent binary classifications. Compared with prior
                   works, the new framework is simple, using standard
                   Transformers and vision backbones, and effective,
                   consistently outperforming all previous works on five
                   multi-label classification data sets, including MS-COCO,
                   PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we
                   establish $91.3\%$ mAP on MS-COCO. We hope its compact
                   structure, simple implementation, and superior performance
                   serve as a strong baseline for multi-label classification
                   tasks and future studies. The code will be available soon at
                   https://github.com/SlongLiu/query2labels.",
  month         =  jul,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2107.10834"
}

@ARTICLE{cma,
author = {You, Renchun and Zhiyao, Guo and Cui, Lei and Long, Xiang and Bao, Yingze and Wen, Shilei},
year = {2020},
month = {04},
pages = {12709-12716},
title = {Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification},
volume = {34},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i07.6964}
}

@INPROCEEDINGS{srn,
author = {F. Zhu and H. Li and W. Ouyang and N. Yu and X. Wang},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Learning Spatial Regularization with Image-Level Supervisions for Multi-label Image Classification},
year = {2017},
volume = {},
issn = {1063-6919},
pages = {2027-2036},
abstract = {Multi-label image classification is a fundamental but challenging task in computer vision. Great progress has been achieved by exploiting semantic relations between labels in recent years. However, conventional approaches are unable to model the underlying spatial relations between labels in multi-label images, because spatial annotations of the labels are generally not provided. In this paper, we propose a unified deep neural network that exploits both semantic and spatial relations between labels with only image-level supervisions. Given a multi-label image, our proposed Spatial Regularization Network (SRN) generates attention maps for all labels and captures the underlying relations between them via learnable convolutions. By aggregating the regularized classification results with original results by a ResNet-101 network, the classification performance can be consistently improved. The whole deep neural network is trained end-to-end with only image-level annotations, thus requires no additional efforts on image annotations. Extensive evaluations on 3 public datasets with different types of labels show that our approach significantly outperforms state-of-the-arts and has strong generalization capability. Analysis of the learned SRN model demonstrates that it can effectively capture both semantic and spatial relations of labels for improving classification performance.},
keywords = {semantics;neural networks;visualization;training;kernel;computer vision},
doi = {10.1109/CVPR.2017.219},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.219},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}

@INPROCEEDINGS{sr-cnn-rnn,
author = {F. Liu and T. Xiang and T. M. Hospedales and W. Yang and C. Sun},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Semantic Regularisation for Recurrent Image Annotation},
year = {2017},
volume = {},
issn = {1063-6919},
pages = {4160-4168},
abstract = {The CNN-RNN design pattern is increasingly widely applied in a variety of image annotation tasks including multi-label classification and captioning. Existing models use the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN. This leaves the RNN overstretched with two jobs: predicting the visual concepts and modelling their correlations for generating structured annotation output. Importantly this makes the end-to-end training of the CNN and RNN slow and ineffective due to the difficulty of back propagating gradients through the RNN to train the CNN. We propose a simple modification to the design pattern that makes learning more effective and efficient. Specifically, we propose to use a semantically regularised embedding layer as the interface between the CNN and RNN. Regularising the interface can partially or completely decouple the learning problems, allowing each to be more effectively trained and jointly training much more efficient. Extensive experiments show that state-of-the art performance is achieved on multi-label classification as well as image captioning.},
keywords = {semantics;training;visualization;correlation;predictive models;decoding},
doi = {10.1109/CVPR.2017.443},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.443},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}

@INPROCEEDINGS{cnn-rnn,
author = {J. Wang and Y. Yang and J. Mao and Z. Huang and C. Huang and W. Xu},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {CNN-RNN: A Unified Framework for Multi-label Image Classification},
year = {2016},
volume = {},
issn = {1063-6919},
pages = {2285-2294},
abstract = {While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification models.},
keywords = {computational modeling;hidden markov models;logic gates;neurons;semantics;redundancy;recurrent neural networks},
doi = {10.1109/CVPR.2016.251},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.251},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@InProceedings{resnet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@INPROCEEDINGS{resnext,
  author={Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={Aggregated Residual Transformations for Deep Neural Networks},
  year={2017},
  volume={},
  number={},
  pages={5987-5995},
  keywords={Complexity theory;Neurons;Topology;Computer architecture;Neural networks;Network topology},
  doi={10.1109/CVPR.2017.634}
}

@ARTICLE{vit,
  title         = "An image is worth 16x16 words: Transformers for image
                   recognition at scale",
  author        = "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov,
                   Alexander and Weissenborn, Dirk and Zhai, Xiaohua and
                   Unterthiner, Thomas and Dehghani, Mostafa and Minderer,
                   Matthias and Heigold, Georg and Gelly, Sylvain and
                   Uszkoreit, Jakob and Houlsby, Neil",
  abstract      = "While the Transformer architecture has become the de-facto
                   standard for natural language processing tasks, its
                   applications to computer vision remain limited. In vision,
                   attention is either applied in conjunction with
                   convolutional networks, or used to replace certain
                   components of convolutional networks while keeping their
                   overall structure in place. We show that this reliance on
                   CNNs is not necessary and a pure transformer applied
                   directly to sequences of image patches can perform very well
                   on image classification tasks. When pre-trained on large
                   amounts of data and transferred to multiple mid-sized or
                   small image recognition benchmarks (ImageNet, CIFAR-100,
                   VTAB, etc.), Vision Transformer (ViT) attains excellent
                   results compared to state-of-the-art convolutional networks
                   while requiring substantially fewer computational resources
                   to train.",
  month         =  oct,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2010.11929"
}

@inproceedings{alexnet,
 author = {Krizhevsky Alex and Sutskever Ilya and Hinton Geoffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@misc{googlenet,
Author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
Title = {Going Deeper with Convolutions},
Year = {2014},
Eprint = {arXiv:1409.4842},
}

@misc{cnn-labeling,
  author = {Yunchao Gong and Yangqing Jia and Thomas Leung and Alexander Toshev and Sergey Ioffe},
  title = {Deep Convolutional Ranking for Multilabel Image Annotation},
  year = {2013},
  eprint = {arXiv:1312.4894},
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Li Deng and Jia Dong and Wei Socher and Richard Li and Li-Jia Li and Kai Fei-Fei},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{cocodataset,
  author    = {Tsung{-}Yi Lin and Michael Maire and Serge J. Belongie and Lubomir D. Bourdev and Ross B. Girshick and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{'{a} }r and C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  archivePrefix = {arXiv},
  eprint    = {1405.0312},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nus-wide-civr09,
  author={Tat-Seng Chua and Jinhui Tang and Richang Hong and Haojie Li and Zhiping Luo and Yan-Tao Zheng},
  booktitle={Proc. of ACM Conf. on Image and Video Retrieval (CIVR'09)},
  posted-at={July 8-10, 2009},
  title={NUS-WIDE: A Real-World Web Image Database from National University of Singapore},
  address={Santorini, Greece. },
  year={July 8-10, 2009}
}

@ARTICLE{mixed-precision,
  title         = "Mixed Precision Training",
  author        = "Micikevicius, Paulius and Narang, Sharan and Alben, Jonah
                   and Diamos, Gregory and Elsen, Erich and Garcia, David and
                   Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii
                   and Venkatesh, Ganesh and Wu, Hao",
  abstract      = "Deep neural networks have enabled progress in a wide variety
                   of applications. Growing the size of the neural network
                   typically results in improved accuracy. As model sizes grow,
                   the memory and compute requirements for training these
                   models also increases. We introduce a technique to train
                   deep neural networks using half precision floating point
                   numbers. In our technique, weights, activations and
                   gradients are stored in IEEE half-precision format.
                   Half-precision floating numbers have limited numerical range
                   compared to single-precision numbers. We propose two
                   techniques to handle this loss of information. Firstly, we
                   recommend maintaining a single-precision copy of the weights
                   that accumulates the gradients after each optimizer step.
                   This single-precision copy is rounded to half-precision
                   format during training. Secondly, we propose scaling the
                   loss appropriately to handle the loss of information with
                   half-precision gradients. We demonstrate that this approach
                   works for a wide variety of models including convolution
                   neural networks, recurrent neural networks and generative
                   adversarial networks. This technique works for large scale
                   models with more than 100 million parameters trained on
                   large datasets. Using this approach, we can reduce the
                   memory consumption of deep learning models by nearly 2x. In
                   future processors, we can also expect a significant
                   computation speedup using half-precision hardware units.",
  month         =  oct,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1710.03740"
}

