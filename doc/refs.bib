@ARTICLE{dnn-cls,
  title         = "Multi-column deep neural networks for image classification",
  author        = "Cire{\c s}an, Dan and Meier, Ueli and Schmidhuber, Juergen",
  abstract      = "Traditional methods of computer vision and machine learning
                   cannot match human performance on tasks such as the
                   recognition of handwritten digits or traffic signs. Our
                   biologically plausible deep artificial neural network
                   architectures can. Small (often minimal) receptive fields of
                   convolutional winner-take-all neurons yield large network
                   depth, resulting in roughly as many sparsely connected
                   neural layers as found in mammals between retina and visual
                   cortex. Only winner neurons are trained. Several deep neural
                   columns become experts on inputs preprocessed in different
                   ways; their predictions are averaged. Graphics cards allow
                   for fast training. On the very competitive MNIST handwriting
                   benchmark, our method is the first to achieve near-human
                   performance. On a traffic sign recognition benchmark it
                   outperforms humans by a factor of two. We also improve the
                   state-of-the-art on a plethora of common image
                   classification benchmarks.",
  month         =  feb,
  year          =  2012,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1202.2745"
}

@ARTICLE{cnn-cls-1,
  title         = "Very deep convolutional networks for large-scale image
                   recognition",
  author        = "Simonyan, Karen and Zisserman, Andrew",
  abstract      = "In this work we investigate the effect of the convolutional
                   network depth on its accuracy in the large-scale image
                   recognition setting. Our main contribution is a thorough
                   evaluation of networks of increasing depth using an
                   architecture with very small (3x3) convolution filters,
                   which shows that a significant improvement on the prior-art
                   configurations can be achieved by pushing the depth to 16-19
                   weight layers. These findings were the basis of our ImageNet
                   Challenge 2014 submission, where our team secured the first
                   and the second places in the localisation and classification
                   tracks respectively. We also show that our representations
                   generalise well to other datasets, where they achieve
                   state-of-the-art results. We have made our two
                   best-performing ConvNet models publicly available to
                   facilitate further research on the use of deep visual
                   representations in computer vision.",
  month         =  sep,
  year          =  2014,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1409.1556"
}


@ARTICLE{cnn-cls-2,
  title         = "An Introduction to Convolutional Neural Networks",
  author        = "O'Shea, Keiron and Nash, Ryan",
  abstract      = "The field of machine learning has taken a dramatic twist in
                   recent times, with the rise of the Artificial Neural Network
                   (ANN). These biologically inspired computational models are
                   able to far exceed the performance of previous forms of
                   artificial intelligence in common machine learning tasks.
                   One of the most impressive forms of ANN architecture is that
                   of the Convolutional Neural Network (CNN). CNNs are
                   primarily used to solve difficult image-driven pattern
                   recognition tasks and with their precise yet simple
                   architecture, offers a simplified method of getting started
                   with ANNs. This document provides a brief introduction to
                   CNNs, discussing recently published papers and newly formed
                   techniques in developing these brilliantly fantastic image
                   recognition models. This introduction assumes you are
                   familiar with the fundamentals of ANNs and machine learning.",
  month         =  nov,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1511.08458"
}


@ARTICLE{cnn-sinn,
  title         = "Learning structured inference neural networks with label
                   relations",
  author        = "Hu, Hexiang and Zhou, Guang-Tong and Deng, Zhiwei and Liao,
                   Zicheng and Mori, Greg",
  abstract      = "Images of scenes have various objects as well as abundant
                   attributes, and diverse levels of visual categorization are
                   possible. A natural image could be assigned with
                   fine-grained labels that describe major components,
                   coarse-grained labels that depict high level abstraction or
                   a set of labels that reveal attributes. Such categorization
                   at different concept layers can be modeled with label graphs
                   encoding label information. In this paper, we exploit this
                   rich information with a state-of-art deep learning
                   framework, and propose a generic structured model that
                   leverages diverse label relations to improve image
                   classification performance. Our approach employs a novel
                   stacked label prediction neural network, capturing both
                   inter-level and intra-level label semantics. We evaluate our
                   method on benchmark image datasets, and empirical results
                   illustrate the efficacy of our model.",
  month         =  nov,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1511.05616"
}

@ARTICLE{cnn-neighbors,
  title         = "Love thy neighbors: Image annotation by exploiting image
                   metadata",
  author        = "Johnson, Justin and Ballan, Lamberto and Li, Fei-Fei",
  abstract      = "Some images that are difficult to recognize on their own may
                   become more clear in the context of a neighborhood of
                   related images with similar social-network metadata. We
                   build on this intuition to improve multilabel image
                   annotation. Our model uses image metadata nonparametrically
                   to generate neighborhoods of related images using Jaccard
                   similarities, then uses a deep neural network to blend
                   visual information from the image and its neighbors. Prior
                   work typically models image metadata parametrically, in
                   contrast, our nonparametric treatment allows our model to
                   perform well even when the vocabulary of metadata changes
                   between training and testing. We perform comprehensive
                   experiments on the NUS-WIDE dataset, where we show that our
                   model outperforms state-of-the-art methods for multilabel
                   image annotation even when our model is forced to generalize
                   to new types of metadata.",
  month         =  aug,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1508.07647"
}

@ARTICLE{cnn-location,
  title         = "Improving image classification with location context",
  author        = "Tang, Kevin and Paluri, Manohar and Fei-Fei, Li and Fergus,
                   Rob and Bourdev, Lubomir",
  abstract      = "With the widespread availability of cellphones and cameras
                   that have GPS capabilities, it is common for images being
                   uploaded to the Internet today to have GPS coordinates
                   associated with them. In addition to research that tries to
                   predict GPS coordinates from visual features, this also
                   opens up the door to problems that are conditioned on the
                   availability of GPS coordinates. In this work, we tackle the
                   problem of performing image classification with location
                   context, in which we are given the GPS coordinates for
                   images in both the train and test phases. We explore
                   different ways of encoding and extracting features from the
                   GPS coordinates, and show how to naturally incorporate these
                   features into a Convolutional Neural Network (CNN), the
                   current state-of-the-art for most image classification and
                   recognition problems. We also show how it is possible to
                   simultaneously learn the optimal pooling radii for a subset
                   of our features within the CNN framework. To evaluate our
                   model and to help promote research in this area, we identify
                   a set of location-sensitive concepts and annotate a subset
                   of the Yahoo Flickr Creative Commons 100M dataset that has
                   GPS coordinates with these concepts, which we make publicly
                   available. By leveraging location context, we are able to
                   achieve almost a 7\% gain in mean average precision.",
  month         =  may,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1505.03873"
}

@ARTICLE{cpsd,
  title         = "Boosting multi-Label Image Classification with complementary
                   Parallel Self-distillation",
  author        = "Xu, Jiazhi and Huang, Sheng and Zhou, Fengtao and Huangfu,
                   Luwen and Zeng, Daniel and Liu, Bo",
  abstract      = "Multi-Label Image Classification (MLIC) approaches usually
                   exploit label correlations to achieve good performance.
                   However, emphasizing correlation like co-occurrence may
                   overlook discriminative features of the target itself and
                   lead to model overfitting, thus undermining the performance.
                   In this study, we propose a generic framework named Parallel
                   Self-Distillation (PSD) for boosting MLIC models. PSD
                   decomposes the original MLIC task into several simpler MLIC
                   sub-tasks via two elaborated complementary task
                   decomposition strategies named Co-occurrence Graph Partition
                   (CGP) and Dis-occurrence Graph Partition (DGP). Then, the
                   MLIC models of fewer categories are trained with these
                   sub-tasks in parallel for respectively learning the joint
                   patterns and the category-specific patterns of labels.
                   Finally, knowledge distillation is leveraged to learn a
                   compact global ensemble of full categories with these
                   learned patterns for reconciling the label correlation
                   exploitation and model overfitting. Extensive results on
                   MS-COCO and NUS-WIDE datasets demonstrate that our framework
                   can be easily plugged into many MLIC approaches and improve
                   performances of recent state-of-the-art approaches. The
                   explainable visual study also further validates that our
                   method is able to learn both the category-specific and
                   co-occurring features. The source code is released at
                   https://github.com/Robbie-Xu/CPSD.",
  month         =  may,
  year          =  2022,
  copyright     = "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2205.10986"
}


@ARTICLE{q2l,
  title         = "{Query2Label}: A Simple Transformer Way to {Multi-Label}
                   Classification",
  author        = "Liu, Shilong and Zhang, Lei and Yang, Xiao and Su, Hang and
                   Zhu, Jun",
  abstract      = "This paper presents a simple and effective approach to
                   solving the multi-label classification problem. The proposed
                   approach leverages Transformer decoders to query the
                   existence of a class label. The use of Transformer is rooted
                   in the need of extracting local discriminative features
                   adaptively for different labels, which is a strongly desired
                   property due to the existence of multiple objects in one
                   image. The built-in cross-attention module in the
                   Transformer decoder offers an effective way to use label
                   embeddings as queries to probe and pool class-related
                   features from a feature map computed by a vision backbone
                   for subsequent binary classifications. Compared with prior
                   works, the new framework is simple, using standard
                   Transformers and vision backbones, and effective,
                   consistently outperforming all previous works on five
                   multi-label classification data sets, including MS-COCO,
                   PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we
                   establish $91.3\%$ mAP on MS-COCO. We hope its compact
                   structure, simple implementation, and superior performance
                   serve as a strong baseline for multi-label classification
                   tasks and future studies. The code will be available soon at
                   https://github.com/SlongLiu/query2labels.",
  month         =  jul,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2107.10834"
}

@ARTICLE{cma,
  title         = "Cross-modality attention with semantic graph embedding for
                   multi-label classification",
  author        = "You, Renchun and Guo, Zhiyao and Cui, Lei and Long, Xiang
                   and Bao, Yingze and Wen, Shilei",
  abstract      = "Multi-label image and video classification are fundamental
                   yet challenging tasks in computer vision. The main
                   challenges lie in capturing spatial or temporal dependencies
                   between labels and discovering the locations of
                   discriminative features for each class. In order to overcome
                   these challenges, we propose to use cross-modality attention
                   with semantic graph embedding for multi label
                   classification. Based on the constructed label graph, we
                   propose an adjacency-based similarity graph embedding method
                   to learn semantic label embeddings, which explicitly exploit
                   label relationships. Then our novel cross-modality attention
                   maps are generated with the guidance of learned label
                   embeddings. Experiments on two multi-label image
                   classification datasets (MS-COCO and NUS-WIDE) show our
                   method outperforms other existing state-of-the-arts. In
                   addition, we validate our method on a large multi-label
                   video classification dataset (YouTube-8M Segments) and the
                   evaluation results demonstrate the generalization capability
                   of our method.",
  month         =  dec,
  year          =  2019,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1912.07872"
}

@ARTICLE{srn,
  title         = "Learning spatial regularization with image-level
                   supervisions for multi-label image classification",
  author        = "Zhu, Feng and Li, Hongsheng and Ouyang, Wanli and Yu,
                   Nenghai and Wang, Xiaogang",
  abstract      = "Multi-label image classification is a fundamental but
                   challenging task in computer vision. Great progress has been
                   achieved by exploiting semantic relations between labels in
                   recent years. However, conventional approaches are unable to
                   model the underlying spatial relations between labels in
                   multi-label images, because spatial annotations of the
                   labels are generally not provided. In this paper, we propose
                   a unified deep neural network that exploits both semantic
                   and spatial relations between labels with only image-level
                   supervisions. Given a multi-label image, our proposed
                   Spatial Regularization Network (SRN) generates attention
                   maps for all labels and captures the underlying relations
                   between them via learnable convolutions. By aggregating the
                   regularized classification results with original results by
                   a ResNet-101 network, the classification performance can be
                   consistently improved. The whole deep neural network is
                   trained end-to-end with only image-level annotations, thus
                   requires no additional efforts on image annotations.
                   Extensive evaluations on 3 public datasets with different
                   types of labels show that our approach significantly
                   outperforms state-of-the-arts and has strong generalization
                   capability. Analysis of the learned SRN model demonstrates
                   that it can effectively capture both semantic and spatial
                   relations of labels for improving classification
                   performance.",
  month         =  feb,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1702.05891"
}

@ARTICLE{sr-cnn-rnn,
  title         = "Semantic Regularisation for Recurrent Image Annotation",
  author        = "Liu, Feng and Xiang, Tao and Hospedales, Timothy M and Yang,
                   Wankou and Sun, Changyin",
  abstract      = "The ``CNN-RNN'' design pattern is increasingly widely
                   applied in a variety of image annotation tasks including
                   multi-label classification and captioning. Existing models
                   use the weakly semantic CNN hidden layer or its transform as
                   the image embedding that provides the interface between the
                   CNN and RNN. This leaves the RNN overstretched with two
                   jobs: predicting the visual concepts and modelling their
                   correlations for generating structured annotation output.
                   Importantly this makes the end-to-end training of the CNN
                   and RNN slow and ineffective due to the difficulty of back
                   propagating gradients through the RNN to train the CNN. We
                   propose a simple modification to the design pattern that
                   makes learning more effective and efficient. Specifically,
                   we propose to use a semantically regularised embedding layer
                   as the interface between the CNN and RNN. Regularising the
                   interface can partially or completely decouple the learning
                   problems, allowing each to be more effectively trained and
                   jointly training much more efficient. Extensive experiments
                   show that state-of-the art performance is achieved on
                   multi-label classification as well as image captioning.",
  month         =  nov,
  year          =  2016,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1611.05490"
}


@ARTICLE{cnn-rnn,
  title         = "{CNN-RNN}: A unified framework for multi-label image
                   classification",
  author        = "Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng
                   and Huang, Chang and Xu, Wei",
  abstract      = "While deep convolutional neural networks (CNNs) have shown a
                   great success in single-label image classification, it is
                   important to note that real world images generally contain
                   multiple labels, which could correspond to different
                   objects, scenes, actions and attributes in an image.
                   Traditional approaches to multi-label image classification
                   learn independent classifiers for each category and employ
                   ranking or thresholding on the classification results. These
                   techniques, although working well, fail to explicitly
                   exploit the label dependencies in an image. In this paper,
                   we utilize recurrent neural networks (RNNs) to address this
                   problem. Combined with CNNs, the proposed CNN-RNN framework
                   learns a joint image-label embedding to characterize the
                   semantic label dependency as well as the image-label
                   relevance, and it can be trained end-to-end from scratch to
                   integrate both information in a unified framework.
                   Experimental results on public benchmark datasets
                   demonstrate that the proposed architecture achieves better
                   performance than the state-of-the-art multi-label
                   classification model",
  month         =  apr,
  year          =  2016,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1604.04573"
}

@ARTICLE{resnet,
  title         = "Deep residual learning for image recognition",
  author        = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                   Jian",
  abstract      = "Deeper neural networks are more difficult to train. We
                   present a residual learning framework to ease the training
                   of networks that are substantially deeper than those used
                   previously. We explicitly reformulate the layers as learning
                   residual functions with reference to the layer inputs,
                   instead of learning unreferenced functions. We provide
                   comprehensive empirical evidence showing that these residual
                   networks are easier to optimize, and can gain accuracy from
                   considerably increased depth. On the ImageNet dataset we
                   evaluate residual nets with a depth of up to 152 layers---8x
                   deeper than VGG nets but still having lower complexity. An
                   ensemble of these residual nets achieves 3.57\% error on the
                   ImageNet test set. This result won the 1st place on the
                   ILSVRC 2015 classification task. We also present analysis on
                   CIFAR-10 with 100 and 1000 layers. The depth of
                   representations is of central importance for many visual
                   recognition tasks. Solely due to our extremely deep
                   representations, we obtain a 28\% relative improvement on
                   the COCO object detection dataset. Deep residual nets are
                   foundations of our submissions to ILSVRC \& COCO 2015
                   competitions, where we also won the 1st places on the tasks
                   of ImageNet detection, ImageNet localization, COCO
                   detection, and COCO segmentation.",
  month         =  dec,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1512.03385"
}

@ARTICLE{resnext,
  title         = "Aggregated residual transformations for deep neural networks",
  author        = "Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and
                   Tu, Zhuowen and He, Kaiming",
  abstract      = "We present a simple, highly modularized network architecture
                   for image classification. Our network is constructed by
                   repeating a building block that aggregates a set of
                   transformations with the same topology. Our simple design
                   results in a homogeneous, multi-branch architecture that has
                   only a few hyper-parameters to set. This strategy exposes a
                   new dimension, which we call ``cardinality'' (the size of
                   the set of transformations), as an essential factor in
                   addition to the dimensions of depth and width. On the
                   ImageNet-1K dataset, we empirically show that even under the
                   restricted condition of maintaining complexity, increasing
                   cardinality is able to improve classification accuracy.
                   Moreover, increasing cardinality is more effective than
                   going deeper or wider when we increase the capacity. Our
                   models, named ResNeXt, are the foundations of our entry to
                   the ILSVRC 2016 classification task in which we secured 2nd
                   place. We further investigate ResNeXt on an ImageNet-5K set
                   and the COCO detection set, also showing better results than
                   its ResNet counterpart. The code and models are publicly
                   available online.",
  month         =  nov,
  year          =  2016,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1611.05431"
}

@ARTICLE{vit,
  title         = "An image is worth 16x16 words: Transformers for image
                   recognition at scale",
  author        = "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov,
                   Alexander and Weissenborn, Dirk and Zhai, Xiaohua and
                   Unterthiner, Thomas and Dehghani, Mostafa and Minderer,
                   Matthias and Heigold, Georg and Gelly, Sylvain and
                   Uszkoreit, Jakob and Houlsby, Neil",
  abstract      = "While the Transformer architecture has become the de-facto
                   standard for natural language processing tasks, its
                   applications to computer vision remain limited. In vision,
                   attention is either applied in conjunction with
                   convolutional networks, or used to replace certain
                   components of convolutional networks while keeping their
                   overall structure in place. We show that this reliance on
                   CNNs is not necessary and a pure transformer applied
                   directly to sequences of image patches can perform very well
                   on image classification tasks. When pre-trained on large
                   amounts of data and transferred to multiple mid-sized or
                   small image recognition benchmarks (ImageNet, CIFAR-100,
                   VTAB, etc.), Vision Transformer (ViT) attains excellent
                   results compared to state-of-the-art convolutional networks
                   while requiring substantially fewer computational resources
                   to train.",
  month         =  oct,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2010.11929"
}


@inproceedings{alexnet,
 author = {Krizhevsky Alex and Sutskever Ilya and Hinton Geoffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@misc{googlenet,
Author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
Title = {Going Deeper with Convolutions},
Year = {2014},
Eprint = {arXiv:1409.4842},
}

@misc{cnn-labeling,
  author = {Yunchao Gong and Yangqing Jia and Thomas Leung and Alexander Toshev and Sergey Ioffe},
  title = {Deep Convolutional Ranking for Multilabel Image Annotation},
  year = {2013},
  eprint = {arXiv:1312.4894},
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Li Deng and Jia Dong and Wei Socher and Richard Li and Li-Jia Li and Kai Fei-Fei},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{cocodataset,
  author    = {Tsung{-}Yi Lin and Michael Maire and Serge J. Belongie and Lubomir D. Bourdev and Ross B. Girshick and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{'{a} }r and C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  archivePrefix = {arXiv},
  eprint    = {1405.0312},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nus-wide-civr09,
  author={Tat-Seng Chua and Jinhui Tang and Richang Hong and Haojie Li and Zhiping Luo and Yan-Tao Zheng},
  booktitle={Proc. of ACM Conf. on Image and Video Retrieval (CIVR'09)},
  posted-at={July 8-10, 2009},
  title={NUS-WIDE: A Real-World Web Image Database from National University of Singapore},
  address={Santorini, Greece. },
  year={July 8-10, 2009}
}

  @ARTICLE{mixed-precision,
  title         = "Mixed Precision Training",
  author        = "Micikevicius, Paulius and Narang, Sharan and Alben, Jonah
                   and Diamos, Gregory and Elsen, Erich and Garcia, David and
                   Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii
                   and Venkatesh, Ganesh and Wu, Hao",
  abstract      = "Deep neural networks have enabled progress in a wide variety
                   of applications. Growing the size of the neural network
                   typically results in improved accuracy. As model sizes grow,
                   the memory and compute requirements for training these
                   models also increases. We introduce a technique to train
                   deep neural networks using half precision floating point
                   numbers. In our technique, weights, activations and
                   gradients are stored in IEEE half-precision format.
                   Half-precision floating numbers have limited numerical range
                   compared to single-precision numbers. We propose two
                   techniques to handle this loss of information. Firstly, we
                   recommend maintaining a single-precision copy of the weights
                   that accumulates the gradients after each optimizer step.
                   This single-precision copy is rounded to half-precision
                   format during training. Secondly, we propose scaling the
                   loss appropriately to handle the loss of information with
                   half-precision gradients. We demonstrate that this approach
                   works for a wide variety of models including convolution
                   neural networks, recurrent neural networks and generative
                   adversarial networks. This technique works for large scale
                   models with more than 100 million parameters trained on
                   large datasets. Using this approach, we can reduce the
                   memory consumption of deep learning models by nearly 2x. In
                   future processors, we can also expect a significant
                   computation speedup using half-precision hardware units.",
  month         =  oct,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1710.03740"
}

